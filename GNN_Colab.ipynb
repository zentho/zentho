{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zentho/zentho/blob/main/GNN_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# Graph Neural Networks with PyTorch Geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "Credits: This homework assignment was directly taken and modified from Stanford's [CS 224W (Machine Learning with Graphs)](https://web.stanford.edu/class/cs224w/index.html).\n",
        "\n",
        "In this colab notebook, we will work to construct our own graph neural network using PyTorch Geometric (PyG) and then apply that model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on two different graph-based tasks: 1) node property prediction, predicting properties of single nodes and 2) graph property prediction, predicting properties of entire graphs or subgraphs.\n",
        "\n",
        "First, we will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
        "\n",
        "Then, we will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
        "\n",
        "Lastly, we will build our own graph neural network using PyTorch Geometric. We will then train and evaluate our model on the OGB node property prediction and graph property prediction tasks.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell.\n",
        "\n",
        "There are 5 questions to be answered throughout this notebook. The first 3 will not be graded and are mostly there to help you understand PyG datasets. The last 2 involve generating csv files with model predictions that will be submitted to the Gradescope autograder.\n",
        "\n",
        "We recommend you save a copy of this colab in your drive so you don't lose progress!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Geometric Tutorial\n",
        "\n",
        "PyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets.\n",
        "\n",
        "This tutorial is adapted from https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by [Matthias Fey](https://rusty1s.github.io/#/)"
      ],
      "metadata": {
        "id": "xJr5W5rz_rrx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCK7krJdp4o8"
      },
      "source": [
        "# Setup\n",
        "The installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkP8pA1qBE5"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hfxJ-qRg2"
      },
      "source": [
        "# Install torch geometric\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Recently, deep learning on graphs has emerged to one of the hottest research fields in the deep learning community.\n",
        "Here, **Graph Neural Networks (GNNs)** aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations.\n",
        "\n",
        "This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the **[PyTorch Geometric (PyG) library](https://github.com/rusty1s/pytorch_geometric)**.\n",
        "PyTorch Geometric is an extension library to the popular deep learning framework [PyTorch](https://pytorch.org/), and consists of various methods and utilities to ease the implementation of Graph Neural Networks."
      ],
      "metadata": {
        "id": "h74noJf5_91g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# 1) PyTorch Geometric (Datasets and Data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "PyTorch Geometric has two classes for storing and/or transforming graphs into a tensor format. One is `torch_geometric.datasets`, which is used to represent entire graph datasets. Another is `torch_geometric.data`, which provides the data handling of indvidual graphs in PyTorch tensors. In this section, we will learn how to use `torch_geometric.datasets` and `torch_geometric.data` together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "## PyG Datasets\n",
        "\n",
        "The `torch_geometric.datasets` (think of it as analogous to `torchvision.datasets` but for graphs instead of computer vision) class has many common graph datasets. Here we will explore its usage through one example dataset.\n",
        "\n",
        "The `ENZYMES` dataset provided by TU Dortmund University contains 600 molecules represented as graphs, with nodes representing individual compounds and edges representing chemical bounds. The class label encodes different molecular properties like toxicity, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT5qca3x6XpG"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "root = './enzymes'\n",
        "name = 'ENZYMES'\n",
        "\n",
        "# The ENZYMES dataset\n",
        "pyg_dataset= TUDataset(root, name)\n",
        "\n",
        "# You will find that there are 600 graphs in this dataset\n",
        "print(pyg_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "## Question 1: What is the number of classes and number of features in the ENZYMES dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iF_Kyqr_JbY"
      },
      "source": [
        "def get_num_classes(pyg_dataset):\n",
        "    # TODO: Implement a function that takes a PyG dataset object\n",
        "    # and returns the number of classes for that dataset.\n",
        "\n",
        "    num_classes = ...\n",
        "\n",
        "    return num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "    # TODO: Implement a function that takes a PyG dataset object\n",
        "    # and returns the number of features for that dataset.\n",
        "\n",
        "    num_features = ...\n",
        "\n",
        "    return num_features\n",
        "\n",
        "num_classes = get_num_classes(pyg_dataset)\n",
        "num_features = get_num_features(pyg_dataset)\n",
        "print(\"{} dataset has {} classes\".format(name, num_classes))\n",
        "print(\"{} dataset has {} features\".format(name, num_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "## PyG Data\n",
        "\n",
        "Each PyG dataset stores a list of `torch_geometric.data.Data` objects, where each `torch_geometric.data.Data` object represents a graph. We can easily get the `Data` object by indexing into the dataset.\n",
        "We can print the data object anytime via `print(data)` to receive a short summary about its attributes and their shapes:\n",
        "```\n",
        "Data(edge_index=[2, 156], x=[34, 64], y=[34])\n",
        "```\n",
        "We can see that this particular `data` object holds 3 attributes:\n",
        "(1) The `edge_index` property holds the information about the **graph connectivity**, *i.e.*, a tuple of source and destination node indices for each edge.\n",
        "PyG further refers to (2) **node features** as `x` (each of the 34 nodes is assigned a 64-dim feature vector), and to (3) **node labels** as `y` (graph-level or node-level ground truth labels of arbitrary shape. For example, `y=[34]` indicates one label per each of the 34 nodes but `y=[1]` would indicate one label for the whole graph).\n",
        "\n",
        "The `data` object also provides some **utility functions** to infer some basic properties of the underlying graph.\n",
        "For example, we can easily infer whether there exists isolated nodes in the graph (*i.e.* there exists no edge to any node), whether the graph contains self-loops (*i.e.*, $(v, v) \\in \\mathbb{E}$), or whether the graph is undirected (*i.e.*, for each edge $(v, w) \\in \\mathbb{E}$ there also exists the edge $(w, v) \\in \\mathbb{E}$).\n",
        "\n",
        "For more information such as what is stored in the `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 2: What is the label of the graph with index 100 in the ENZYMES dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIis9oTZAfs3"
      },
      "source": [
        "def get_graph_class(pyg_dataset, idx):\n",
        "    # TODO: Implement a function that takes a PyG dataset object,\n",
        "    # an index of a graph within the dataset, and returns the class/label\n",
        "    # of the graph (as an integer).\n",
        "\n",
        "    label = ...\n",
        "\n",
        "    return label\n",
        "\n",
        "# Here pyg_dataset is a dataset for graph classification\n",
        "graph_0 = pyg_dataset[0]\n",
        "print(graph_0)\n",
        "idx = 100\n",
        "label = get_graph_class(pyg_dataset, idx)\n",
        "print('Graph with index {} has label {}'.format(idx, label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't worry about what the numerical value of the label corresponds to. The only thing we care about is that the label is an integer in the range `0 <= label < num_lcasses`."
      ],
      "metadata": {
        "id": "21y_W8zv79n_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Index"
      ],
      "metadata": {
        "id": "3v_CxuGaSPry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll print the `edge_index` attribute of our graph:"
      ],
      "metadata": {
        "id": "Jj_-4fRASS-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = graph_0.edge_index\n",
        "print(edge_index.t())"
      ],
      "metadata": {
        "id": "YDdt7l2USZ18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By printing `edge_index`, we can further understand how PyG represents graph connectivity internally.\n",
        "We can see that for each edge, `edge_index` holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge.\n",
        "\n",
        "This representation is known as the **COO format (coordinate format)** commonly used for representing sparse matrices.\n",
        "Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathbb{V}| \\times |\\mathbb{V}|}$ adjacency matrix, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero."
      ],
      "metadata": {
        "id": "yg1-cbrMS4db"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "## Question 3: How many edges does the graph with index 200 have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5m2DOfhBtWv"
      },
      "source": [
        "def get_graph_num_edges(pyg_dataset, idx):\n",
        "    # TODO: Implement a function that takes a PyG dataset object,\n",
        "    # the index of a graph in the dataset, and returns the number of\n",
        "    # edges in the graph (as an integer). You should not count an edge\n",
        "    # twice if the graph is undirected. For example, in an undirected\n",
        "    # graph G, if two nodes v and u are connected by an edge, this edge\n",
        "    # should only be counted once.\n",
        "    # Note: You can't return the data.num_edges directly\n",
        "\n",
        "    num_edges = ...\n",
        "\n",
        "    return num_edges\n",
        "\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
        "print('Graph with index {} has {} edges'.format(idx, num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# 2) Open Graph Benchmark (OGB)\n",
        "\n",
        "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can then be evaluated by using the OGB Evaluator in a unified manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Dataset and Data\n",
        "\n",
        "OGB also supports PyG dataset and data classes. Here we take a look on the `ogbn-arxiv` dataset. The `ogbn-arxiv` dataset is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract.\n",
        "\n",
        "The task is to predict the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS, which are manually determined (i.e., labeled) by the paper’s authors and arXiv moderators. With the volume of scientific publications doubling every 12 years over the past century, it is practically important to automatically classify each publication’s areas and topics. Formally, the task is to predict the primary categories of the arXiv papers, which is formulated as a 40-class classification problem.\n",
        "\n",
        "Unlike many graph datasets, which are a collection of many invdividual graphs, the `ogbn-arxiv` is just a single graph where each node is considered a separate datapoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpc6bTm3GF02"
      },
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "# Load the dataset and transform it to sparse tensor\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "# Extract the graph\n",
        "data = dataset[0]\n",
        "row, col, edge_attr = data.adj_t.t().coo()\n",
        "data.edge_index = torch.stack([row, col], dim=0)\n",
        "print(data)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "data = data.to(device)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "# 3) GNN: Node Property Prediction\n",
        "\n",
        "In this section we will build our first graph neural network using PyTorch Geometric! Then we will apply it to the task of node property prediction (node classification).\n",
        "\n",
        "## Implementing Graph Neural Networks (GNNs)\n",
        "\n",
        "For this, we will use one of the most simple GNN operators, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)). You can think of this layer as our PyTorch abstraction of the message-passing algorithm described in class (to be completely precise, it uses the paradigm of massage-passing but it's a bit more complicated than that).\n",
        "\n",
        "PyG implements this layer via [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv), which can be executed by passing in the node feature representation `x` and the COO graph connectivity representation `edge_index`.\n",
        "\n",
        "### What is the output of a GNN?\n",
        "\n",
        "The goal of a GNN is to take an input graph $G = (\\mathbb{V}, \\mathbb{E})$ where each node $v_i \\in \\mathbb{V}$ has an input feature vector $X_i^{(0)}$. What we want to learn is a function $f \\to \\mathbb{V} \\times \\mathbb{R}^d$, a function that takes in a node and its feature vector, as well as the graph structure, and outputs an _embedding_, a vector that represents that node in a way that's useful to our downstream task. Once we've mapped nodes and their initial features to their learned embeddings, we can use those embeddings to do a variety of different tasks including node-level, edge-level, or graph-level regression/classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DCtgcHpGIpd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "import pandas as pd\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our first GNN model! Specifically, we will use GCN layers as the foundation for our graph neural network. To do so, we will work with PyG's built-in `GCNConv` layer. Follow the instructions below to define all the networks layers and the forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
        "        ## 2. self.convs has num_layers GCNConv layers\n",
        "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
        "        ## 4. The parameters you can set for GCNConv include 'in_channels' and\n",
        "        ## 'out_channels'. For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
        "        ## 5. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
        "        #########################################\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = ...\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = ...\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        This function takes in\n",
        "        1. a node feature matrix x of shape (V, num_features) where V is the number of\n",
        "        nodes in the graph and num_features = input_dim\n",
        "        2. an edge_index matrix of shape (2, E) where E is the number of edges in the graph.\n",
        "\n",
        "        The output should be a node feature matrix of shape (V, output_dim) where, again,\n",
        "        V is the number of nodes in the graph.\n",
        "\n",
        "        Your forward pass should look like the follows:\n",
        "        - Repeat num_layers - 1 times:\n",
        "            - GCNConv -> BatchNorm -> ReLU -> Dropout\n",
        "        - One final GCNConv at the end\n",
        "        - Return the output of the final GCNConv layer\n",
        "        \"\"\"\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Construct the network as shown in the figure\n",
        "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
        "        ## 3. Don't forget to set F.dropout training to self.training\n",
        "        #########################################\n",
        "\n",
        "        out = ...\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    # TODO: Implement a function that trains the model by\n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## Note:\n",
        "    ## 1. Zero grad the optimizer\n",
        "    ## 2. Feed the data into the model\n",
        "    ## 3. Slice the model output and label by train_idx\n",
        "    ## 4. Feed the sliced output and label to loss_fn\n",
        "    ## You might have to squeeze the labels before\n",
        "    ## passing them into the loss function\n",
        "    #########################################\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    # TODO: Implement a function that tests the model by\n",
        "    # using the given split_idx and evaluator.\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    # Note: no index slicing here\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7F46xkuLiOL"
      },
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "source": [
        "model = GNN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5O5cnPPdVF"
      },
      "source": [
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "print('Evaluating a randomly initialized model')\n",
        "result = test(model, data, split_idx, evaluator)\n",
        "train_acc, valid_acc = result\n",
        "print(f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}%')\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQtt-EKA8P4r"
      },
      "source": [
        "## Question 4: What are your `best_model` validation and test accuracies?\n",
        "\n",
        "Run the cell below to see the results of your best model on the training and validation sets, and to save your model's predictions on the test set to a file named *ogbn-arxiv_node.csv*. You can view this file by clicking on the *Folder* icon on the left side pannel. Submit this file to the Gradescope autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcextqOL2FX"
      },
      "source": [
        "best_result = test(best_model, data, split_idx, evaluator)\n",
        "train_acc, valid_acc = best_result\n",
        "\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}%')\n",
        "\n",
        "best_model.eval()\n",
        "out = best_model(data.x, data.edge_index)\n",
        "y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "print (\"Saving Model Predictions\")\n",
        "\n",
        "preds = {}\n",
        "preds['y_pred'] = y_pred[split_idx['test']].view(-1).cpu().detach().numpy()\n",
        "\n",
        "df = pd.DataFrame(data=preds)\n",
        "# Save locally as csv\n",
        "df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pOD6y80TyI"
      },
      "source": [
        "# 4) GNN: Graph Property Prediction\n",
        "\n",
        "In this section we will create a graph neural network for graph property prediction (i.e., graph-level classification). We will use the `ogbg-molhiv` dataset, which is a small molecular property prediction dataset. Each graph in this dataset represents a molecule, where nodes are atoms, and edges are chemical bonds. Input node features are 9-dimensional, containing properties like atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring or not.\n",
        "\n",
        "The task is to predict the target molecular properties as accurately as possible, where the molecular properties are cast as binary labels, e.g, whether a molecule inhibits HIV virus replication or not.\n",
        "\n",
        "Unlike the dataset above, where we used classification accuracy as a metric to evaluate the performance of our GNN, we will use AUROC (area under the ROC curve) for this new dataset instead since this is a binary classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRg5VOEdQTa4"
      },
      "source": [
        "## Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXb-O5QUIgTH"
      },
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "# Check task type\n",
        "print('Task type: {}'.format(dataset.task_type))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cHHbgW1c5hi"
      },
      "source": [
        "# Load the dataset splits into corresponding dataloaders\n",
        "# We will train the graph classification task on a batch of 32 graphs\n",
        "# Shuffle the order of graphs for training set\n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WLhguSTeazy"
      },
      "source": [
        "## Graph Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05Z14TRYPGn"
      },
      "source": [
        "### Graph Mini-Batching\n",
        "Before diving into the actual model, we introduce the concept of mini-batching with graphs. In order to parallelize the processing of a mini-batch of graphs, PyG combines the graphs into a single disconnected graph data object (`torch_geometric.data.Batch`). The `torch_geometric.data.Batch` class inherits from `torch_geometric.data.Data` (introduced earlier) and contains an additional attribute called `batch`.\n",
        "\n",
        "The `batch` attribute is a vector mapping each node to the index of its corresponding graph within the mini-batch:\n",
        "\n",
        "    batch = [0, ..., 0, 1, ..., n - 2, n - 1, ..., n - 1]\n",
        "\n",
        "This attribute is crucial for associating which graph each node belongs to and can be used to e.g. average the node embeddings for each graph individually to compute graph level embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcic9NNU3nGK"
      },
      "source": [
        "### Implemention\n",
        "Now, we have all of the tools to implement a Graph Prediction model!  \n",
        "\n",
        "We will reuse the existing GNN model to generate `node_embeddings` (i.e., node-level features) and then use  `Global Pooling` over the nodes to create graph level embeddings (i.e., graph-level features) that can be used to predict properties for the each graph. Remember that the `batch` attribute will be essential for performining Global Pooling over our mini-batch of graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Kq3zyjeZ22"
      },
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "### GCN to predict graph property\n",
        "class GNN_Graph(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load encoders for Atoms in molecule graphs\n",
        "        self.node_encoder = AtomEncoder(hidden_dim)\n",
        "\n",
        "        # Node embedding model\n",
        "        # Note that the input_dim and output_dim are set to hidden_dim\n",
        "        self.gnn_node = GNN(hidden_dim, hidden_dim, hidden_dim, num_layers, dropout)\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## Initialize self.pool as a global mean pooling layer\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
        "        #########################################\n",
        "\n",
        "        self.pool = ...\n",
        "\n",
        "        # Output layer\n",
        "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.gnn_node.reset_parameters()\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        # TODO: Implement a function that takes as input a\n",
        "        # mini-batch of graphs (torch_geometric.data.Batch) and\n",
        "        # returns the predicted graph property for each graph.\n",
        "        #\n",
        "        # NOTE: Since we are predicting graph level properties,\n",
        "        # your output will be a tensor with dimension equaling\n",
        "        # the number of graphs in the mini-batch\n",
        "\n",
        "\n",
        "        # Extract important attributes of our mini-batch\n",
        "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
        "        embed = self.node_encoder(x)\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Construct node embeddings using the existing GNN model\n",
        "        ## 2. Use the global pooling layer to aggregate features for each individual graph\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
        "        ## 3. Use a linear layer to predict each graph's property\n",
        "        #########################################\n",
        "\n",
        "        out = ...\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJjnGuMSbjX0"
      },
      "source": [
        "def train(model, device, data_loader, optimizer, loss_fn):\n",
        "    # TODO: Implement a function that trains your model by\n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            ## ignore nan targets (unlabeled) when computing training loss.\n",
        "            is_labeled = batch.y == batch.y\n",
        "\n",
        "            ############# Your code here ############\n",
        "            ## Note:\n",
        "            ## 1. Zero grad the optimizer\n",
        "            ## 2. Feed the data into the model\n",
        "            ## 3. Use `is_labeled` mask to filter output and labels\n",
        "            ## 4. You may need to change the type of label to torch.float32\n",
        "            ## 5. Feed the output and label to the loss_fn\n",
        "            #########################################\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztPHXq_Gzn7U"
      },
      "source": [
        "# The evaluation function\n",
        "def eval(model, device, loader, evaluator):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch)\n",
        "\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYrSnOj0Y4DK"
      },
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 6,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.2,\n",
        "    'lr': 0.0001,\n",
        "    'epochs': 30,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR1wQ4hMZeMw"
      },
      "source": [
        "model = GNN_Graph(args['hidden_dim'],\n",
        "            dataset.num_tasks, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJGTNZiuZy0A"
      },
      "source": [
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "print('Evaluating a randomly initialized model')\n",
        "train_result = eval(model, device, train_loader, evaluator)\n",
        "val_result = eval(model, device, valid_loader, evaluator)\n",
        "print(f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}%')\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    print('Training...')\n",
        "    loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "    print('Evaluating...')\n",
        "    train_result = eval(model, device, train_loader, evaluator)\n",
        "    val_result = eval(model, device, valid_loader, evaluator)\n",
        "\n",
        "    train_acc, valid_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric]\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I17-Qso_n88"
      },
      "source": [
        "## Question 5: What are your `best_model` validation and test ROC-AUC scores?\n",
        "\n",
        "Run the cell below to see the results of your best model on the training and validation sets, and to save your model's predictions on the test set to a file named `ogbg-molhiv_graph.csv`. Again, you can view the file by clicking on the *Folder* icon on the left side pannel. Submit both files to the Gradescope autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq5QaG21dOOO"
      },
      "source": [
        "train_auroc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "valid_auroc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\n",
        "\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_auroc:.2f}%, '\n",
        "      f'Valid: {100 * valid_auroc:.2f}%')\n",
        "\n",
        "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    for step, batch in enumerate(tqdm(test_loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            pred = model(batch)\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "print(\"Saving Model Predictions\")\n",
        "\n",
        "preds = {}\n",
        "preds['y_pred'] = y_pred.reshape(-1)\n",
        "\n",
        "df = pd.DataFrame(data=preds)\n",
        "# Save to csv\n",
        "df.to_csv('ogbg-molhiv_graph.csv', sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}